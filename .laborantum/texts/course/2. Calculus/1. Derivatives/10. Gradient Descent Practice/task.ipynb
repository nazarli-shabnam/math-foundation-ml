{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gradient Descent Practice"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Gradient Descent is the core numerical optimization technique that is used in Machine Learning. In this practice we are going to code the Gradient Descent Algorithm and use it on 1D and 2D functions.\n",
                "\n",
                "Exactly the same way it works with higher dimensional functions with the only exception that it is impossible to visualize the process."
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import json_tricks\n",
                "\n",
                "answer = {}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 1. Code Gradient Descent algorith."
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The inputs into your function are:\n",
                "- current position $\\mathbf x$, `x`\n",
                "- gradient of the objective function $\\nabla L$, `grad`\n",
                "- learning rate $\\alpha$\n",
                "\n",
                "Code the update step of Gradient Descent algorithm."
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def grad_descent(x, grad, alpha=0.1):\n",
                "    res = x\n",
                "    ## YOUR CODE HERE\n",
                "    return res"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 2. Code Objective Function"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will use a super simple objective function:\n",
                "\n",
                "$f(x) = x^2$\n",
                "\n",
                "Code this function and its gradient below."
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def f_1d(x):\n",
                "    res = x\n",
                "    ## YOUR CODE HERE\n",
                "    return res\n",
                "\n",
                "def grad_f_1d(x):\n",
                "    res = x\n",
                "    ## YOUR CODE HERE\n",
                "    return res\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "position = 5\n",
                "\n",
                "history = []\n",
                "for index in range(10):\n",
                "    grad = grad_f_1d(position)\n",
                "    position = grad_descent(position, grad)\n",
                "    history.append(position)\n",
                "\n",
                "xs = np.linspace(-6, 6, 100)\n",
                "ys = f_1d(xs)\n",
                "\n",
                "history = np.array(history)\n",
                "\n",
                "plt.plot(xs, f_1d(xs), label=\"Loss function\")\n",
                "plt.plot(history, f_1d(history), 'o', label=\"Optimization path\")\n",
                "dx = np.diff(history)  # Change in x\n",
                "dy = np.diff(f_1d(history))       # Change in y\n",
                "plt.quiver(history[:-1], f_1d(history)[:-1], dx, dy, angles=\"xy\", scale_units=\"xy\", scale=1, color=\"red\", label=\"Steps\")\n",
                "\n",
                "plt.legend()\n",
                "plt.title(\"Optimization Path with Arrows\")\n",
                "plt.show()\n",
                "\n",
                "answer['1d'] = history.tolist()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 3. 2D optimization\n",
                "\n",
                "The simple test case is passed. Let us take a look at a 2D function and how is it going to be optimized.\n",
                "\n",
                "In this case we will be optimizing a slightly more sophisticated function:\n",
                "\n",
                "$f(x, y) = (1 - x)^2 + 10 (y - x^2)^2$\n",
                "\n",
                "Note that in this case, the input in loss function is a 2D point with coordinates:\n",
                "\n",
                "```\n",
                "numpy.array([1, 2])\n",
                "```\n",
                "\n",
                "Write a function that calculates the objective function and its gradient"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loss_2d(x):\n",
                "    res = x.sum()\n",
                "    ## YOUR CODE HERE\n",
                "    return res\n",
                "\n",
                "def loss_2d_grad(x):\n",
                "    grad = np.zeros_like(x)\n",
                "    ## YOUR CODE HERE\n",
                "    return grad"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "position = np.array([-1, 0])\n",
                "\n",
                "history = [position]\n",
                "for index in range(1000):\n",
                "    grad = loss_2d_grad(position)\n",
                "    position = grad_descent(position, grad, alpha=0.02)\n",
                "    history.append(position)\n",
                "\n",
                "xs = np.linspace(-2, 2, 400)\n",
                "ys = np.linspace(-1, 3, 400)\n",
                "\n",
                "xs, ys = np.meshgrid(xs, ys)\n",
                "\n",
                "history = np.array(history)\n",
                "\n",
                "plt.contour(xs, ys, loss_2d(np.stack([xs, ys])), label=\"Loss function\", levels=np.logspace(0, 3, 20))\n",
                "x, y = history[:, 0], history[:, 1]\n",
                "dx = np.diff(x)\n",
                "dy = np.diff(y)\n",
                "plt.quiver(x[:-1], y[:-1], dx, dy, angles=\"xy\", scale_units=\"xy\", scale=1, color=\"red\", width=0.005)\n",
                "plt.plot(x, y, 'o', label=\"Optimization path\")\n",
                "\n",
                "answer['2d'] = history.tolist()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Afterword\n",
                "\n",
                "That is really impressive! We have coded one of the two main algorithms in Deep Learning: a Gradient Descent!\n",
                "\n",
                "- How does the gradient descent work? Is its behavior logical?\n",
                "- Which of the steps can potentially be dangerous?\n",
                "- How would you improve Gradient Descent so that there is no that unwanted dangerous step?\n"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "json_tricks.dump(answer, '.answer.json')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}